Voice is important part of communication,
communication is key to any business. Today most of
business cannot run without interaction and organizations
are struggling today to drive customer satisfaction delight
through their customer support and contact centers, this is
due to lack of any kind of measurement, today with the
help of AI and machine learning algorithms, it's possible to
measure etiquettes through voice modulations and
pitch.The goal of this proposed study is to find out how the
various speakers in the conversation felt. We investigated
several strategies for speaker discrimination and sentiment
analysis, as well as accuracy tests on 1D and 2D CNN using
the whole MFCC and Mel-Spectrogram w-w/o
augmentation approaches, in order to find an efficient
Model. We used (TESS,SAVEE,RAVDESS,CREAMA-D)
datasets to train the model to classify gender and emotions,
and we concentrated on balanced audio datasets including
male and female speakers with diverse sentiments. We
observed superior model accuracy on 2D CNN on the
complete MFCC without augmentation (67.38 percent at
50 epoch) and used the f1-score as an evaluation metric,
which yielded a weighted average of 69 on the test set and
the best results on the “female pleasant surprise” and
“angry” class with a score of 1.00 & 0.78.
